{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM model\n",
    "This notebook deals with training & testing an LSTM model for bitcoin price prediction. We'll use PyTorch and particularly PyTorch Lightning modules to build the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries and read data\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('data/raw.csv').drop(columns='market_caps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LSTM takes data in the format `(batch_len, seq_len, n_features)`, so we'll do some data preprocessing to get it into that format (soon). In this case, the batch length is the number of labeled samples, and the sequence length is the number of timesteps per sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing (again)\n",
    "# our data is hourly, so we'll use 24 hour sequences to predict\n",
    "# our batch length will be len(df) - 24, and we have 2 features\n",
    "seq_len = 24\n",
    "batch_len = len(df) - seq_len\n",
    "n_features = 2\n",
    "\n",
    "# TODO: relabel y's to be within {0, 1, 2} (since cross entropy loss expects indices to classes)\n",
    "labels = [-1, 0, 1, -1, 0]\n",
    "y = list(map(lambda x: 2 if x == -1 else x, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing & misc. work\n",
    "fake_data = np.ones((2, seq_len, n_features))\n",
    "t = torch.Tensor(fake_data)\n",
    "lstm = nn.LSTM(input_size=n_features, hidden_size=1, batch_first=True)\n",
    "out, _ = lstm(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lightning module structure\n",
    "class LSTM_Classifier(pl.LightningModule):\n",
    "    def __init__(self, n_features, hidden, seq_len):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden = hidden\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        # lstm layer and linear hidden-state to classes layer\n",
    "        # lstm inputs a batch of samples of shape (seq_len, n_features),\n",
    "        #   outputs 1 hidden state of shape (seq_len, hidden_size)\n",
    "        self.lstm = nn.LSTM(input_size=n_features, hidden_size=2, batch_first=True)\n",
    "        self.h2c = nn.Linear(hidden * seq_len, 3)\n",
    "    \n",
    "    # forward step - classification\n",
    "    def forward(self, X):\n",
    "        lstm_out, _ = self.lstm(X)\n",
    "        class_preds = self.h2c(lstm_out[-1].view((self.hidden * self.seq_len, -1)))\n",
    "        return F.softmax(class_preds)\n",
    "        \n",
    "    def training_step(self, batch):\n",
    "        X, y = batch\n",
    "        y_hat = self.forward(X)\n",
    "        \n",
    "        train_loss = F.cross_entropy(y_hat.view((1, 3)), y)\n",
    "        self.log(train_loss)\n",
    "        return train_loss\n",
    "    \n",
    "    def test_step(self, batch):\n",
    "        X, y = batch\n",
    "        y_hat = self.forward(X)\n",
    "        \n",
    "        test_loss = F.cross_entropy(y_hat.view((1, 3)), y)\n",
    "        self.log(test_loss)\n",
    "        return test_loss\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes/TODO:\n",
    "- Softmax is good for multiclass, and works well with cross-entropy loss. However, the torch cross-entropy loss fn. expects scores $0 \\leq s \\leq 1$ for each possible class. I've added a linear layer that maps LSTM output to a score for each class, and applied the softmax activation fn. to it. I also changed the label for \"buy\" from -1 to 2, to be within the expected $0 \\leq l \\leq C - 1$ range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
