{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM model\n",
    "This notebook deals with training & testing an LSTM model for bitcoin price prediction. We'll use PyTorch and particularly PyTorch Lightning modules to build the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries and read data\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# df = pd.read_csv('data/raw.csv').drop(columns='market_caps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing & Setup\n",
    "The LSTM takes data in the format `(batch_len, seq_len, n_features)`, so we'll do some data preprocessing to get it into that format (soon). In this case, the batch length is the number of labeled samples, and the sequence length is the number of timesteps per sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BTCDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, train_file):\n",
    "        X_raw = np.load(train_file)\n",
    "        scaler = MinMaxScaler()\n",
    "        X_scaled = scaler.fit_transform(X_raw)\n",
    "\n",
    "        # Step 2: Label based on average of next <avg_window> open prices, & truncate data to match\n",
    "        # Labels are {-1, 1} = {sell, buy}; as class indices, these become {0, 1} = {sell, buy}\n",
    "        avg_window = 10 # can't be longer than sequence length\n",
    "        windowed_size = len(X_scaled) - avg_window\n",
    "        avgs = np.zeros(windowed_size)\n",
    "        labels = [0]*windowed_size\n",
    "        for i in range(windowed_size):\n",
    "            avgs[i] = np.average(X_scaled[i:i + avg_window, 0])\n",
    "            labels[i] = int(np.sign(avgs[i] - X_scaled[i, 0]))\n",
    "            if(labels[i] < 0): labels[i] = 0\n",
    "\n",
    "        # columns = ['open', 'high', 'low', 'close', 'volume', 'EMA diff']\n",
    "\n",
    "        # Step 3: divide into sequences and batch\n",
    "        self.seq_len = 60\n",
    "        batch_len = len(X_scaled) - seq_len\n",
    "        self.n_features = 6\n",
    "        X_batched = np.zeros((batch_len, self.seq_len, self.n_features))\n",
    "        for i in range(batch_len):\n",
    "            X_batched[i] = X_scaled[i:i + self.seq_len]\n",
    "\n",
    "        self.x_data = torch.Tensor(X_batched)\n",
    "        self.y_data = torch.Tensor(labels).type(torch.LongTensor)\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        X = self.x_data[idx]\n",
    "        y = self.y_data[idx]\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data at random into train/test sets, since we've fixed all the time-dependencies\n",
    "class BTCData(pl.LightningDataModule):\n",
    "    def __init__(self, filename):\n",
    "        super().__init__()\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.filename = filename\n",
    "        self.batch_size = 1\n",
    "        \n",
    "    def load_data(self):\n",
    "        return np.load(self.filename)\n",
    "    \n",
    "    def make_dataset(self, X, y):\n",
    "        X = torch.Tensor(X)\n",
    "        y = torch.Tensor(y).type(torch.LongTensor)\n",
    "        \n",
    "        return TensorDataset(X, y)\n",
    "        \n",
    "    def setup(self, stage):\n",
    "        data = self.load_data()\n",
    "        data = self.scaler.fit_transform(data)\n",
    "        \n",
    "        avg_window = 10 # can't be longer than sequence length\n",
    "        windowed_size = len(data) - avg_window\n",
    "        avgs = np.zeros(windowed_size)\n",
    "        labels = [0]*windowed_size\n",
    "        for i in range(windowed_size):\n",
    "            avgs[i] = np.average(data[i:i + avg_window, 0])\n",
    "            labels[i] = int(np.sign(avgs[i] - data[i, 0]))\n",
    "            if(labels[i] < 0): labels[i] = 0\n",
    "                \n",
    "        self.seq_len = 60\n",
    "        batch_len = len(data) - seq_len\n",
    "        self.n_features = 6\n",
    "        X_batched = np.zeros((batch_len, self.seq_len, self.n_features))\n",
    "        for i in range(batch_len):\n",
    "            X_batched[i] = data[i:i + self.seq_len]\n",
    "        labels = labels[:batch_len]\n",
    "            \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_batched, labels, test_size=0.2)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.125)\n",
    "        \n",
    "        self.train = self.make_dataset(X_train, y_train)\n",
    "        self.val = self.make_dataset(X_val, y_val)\n",
    "        self.test = self.make_dataset(X_test, y_test)\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train, batch_size=self.batch_size)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test, batch_size=self.batch_size)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val, batch_size=self.batch_size)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           \r"
     ]
    }
   ],
   "source": [
    "# lightning module structure\n",
    "class LSTM_Classifier(pl.LightningModule):\n",
    "    def __init__(self, n_features, hidden, seq_len, classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden = hidden\n",
    "        self.seq_len = seq_len\n",
    "        self.n_classes = classes\n",
    "        \n",
    "        # lstm layer and linear hidden-state to classes layer\n",
    "        # lstm inputs a batch of samples of shape (seq_len, n_features),\n",
    "        #   outputs 1 hidden state of shape (seq_len, hidden_size)\n",
    "        self.lstm = nn.LSTM(input_size=n_features, hidden_size=2, batch_first=True)\n",
    "        # change the output to 2\n",
    "        self.h2c = nn.Linear(hidden * seq_len, self.n_classes)\n",
    "    \n",
    "    def accuracy(self, y_hat, y):\n",
    "        return (y == y_hat.round()).to(torch.float32).mean()\n",
    "    \n",
    "    # forward step - classification\n",
    "    def forward(self, X):\n",
    "        lstm_out, _ = self.lstm(X)\n",
    "        class_preds = self.h2c(lstm_out[-1].view((-1, self.hidden * self.seq_len)))\n",
    "        return F.softmax(class_preds, -1)\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        y_hat = self.forward(X)\n",
    "        \n",
    "        train_loss = F.cross_entropy(y_hat.view((1, self.n_classes)), y)\n",
    "        acc = self.accuracy(y_hat, y)\n",
    "        result = pl.TrainResult(train_loss)\n",
    "        result.log('train_loss', train_loss)\n",
    "        result.log('train_accuracy', acc, prog_bar=True)\n",
    "        return train_loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        y_hat = self.forward(X)\n",
    "        \n",
    "        val_loss = F.cross_entropy(y_hat.view((1, self.n_classes)), y)\n",
    "        acc = self.accuracy(y_hat, y)\n",
    "        result = pl.EvalResult(val_loss)\n",
    "        result.log('val_loss', val_loss)\n",
    "        result.log('val_accuracy', acc, prog_bar=True)\n",
    "        return val_loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        y_hat = self.forward(X)\n",
    "        \n",
    "        test_loss = F.cross_entropy(y_hat.view((1, self.n_classes)), y)\n",
    "        acc = self.accuracy(y_hat, y)\n",
    "        result = pl.EvalResult(test_loss)\n",
    "        result.log('test_loss', test_loss)\n",
    "        result.log('train_accuracy', acc, prog_bar=True)\n",
    "        return test_loss\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes/TODO:\n",
    "- Softmax is good for multiclass, and works well with cross-entropy loss. However, the torch cross-entropy loss fn. expects scores $0 \\leq s \\leq 1$ for each possible class. I've added a linear layer that maps LSTM output to a score for each class, and applied the softmax activation fn. to it. I also changed the label for \"buy\" from -1 to 2, to be within the expected $0 \\leq l \\leq C - 1$ range.\n",
    "- Apparently I need to add an explicit dimension for softmax - this might just be a -1 at the end of the call\n",
    "- I should also implement validation for the LSTM, so it's not just train & test & hope for the best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "Now we're ready to actually train the model. I'll use the PL Trainer module to handle training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "pl.seed_everything(4622)\n",
    "\n",
    "# feature params\n",
    "n_features = 6\n",
    "hidden = 2\n",
    "seq_len = 60\n",
    "classes = 2\n",
    "\n",
    "# instantiate dataset, dataloader, model, and trainer\n",
    "logger = CSVLogger(\"logs\", name=\"lstm\")\n",
    "train_ds = BTCData('data/LSTM_EMA_diff_data.npy')\n",
    "lstm_model = LSTM_Classifier(n_features, hidden, seq_len, classes)\n",
    "trainer = pl.Trainer(deterministic=True, logger=logger, gpus=int(torch.cuda.is_available()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name | Type   | Params\n",
      "--------------------------------\n",
      "0 | lstm | LSTM   | 80    \n",
      "1 | h2c  | Linear | 242   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|▏                                              | 743/150433 [00:09<32:33, 76.62it/s, loss=0.700, v_num=6]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kaih2\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:37: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Saving latest checkpoint..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|▏                                              | 743/150433 [00:09<32:36, 76.49it/s, loss=0.700, v_num=6]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit(lstm_model, train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'models/lstm_done_poorly.pt'\n",
    "torch.save(lstm_model, save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
